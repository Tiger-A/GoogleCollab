{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CRMeSqo0Lpfw"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fca46282be1e40018388ea9ca2342add": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f846d2a7a9c5428b9f032d12acabe04e",
              "IPY_MODEL_b33ce174730f48b7a532f88ddf122200",
              "IPY_MODEL_6014189eb6a74ea1b00166746d60a216"
            ],
            "layout": "IPY_MODEL_bfe8bf8a264c41c98bed89313e893c32"
          }
        },
        "f846d2a7a9c5428b9f032d12acabe04e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Введите пароль:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ac313144d8fc4943a81cbb48f38a7224",
            "placeholder": "​",
            "style": "IPY_MODEL_f564498ffeb944269b62b25de806bda7",
            "value": "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"
          }
        },
        "b33ce174730f48b7a532f88ddf122200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Авторизация",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e9a7a4cc47574386acfaef07ea88892c",
            "style": "IPY_MODEL_ad2389d0a73c4b53aef201c9240bed5f",
            "tooltip": ""
          }
        },
        "6014189eb6a74ea1b00166746d60a216": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2881bd8e35f6442eaf3a4b8e601b69b5",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\u001b[92m\u001b[1mКлюч сохранен!\u001b[0m\n"
                ]
              }
            ]
          }
        },
        "bfe8bf8a264c41c98bed89313e893c32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac313144d8fc4943a81cbb48f38a7224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "none",
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "500px"
          }
        },
        "f564498ffeb944269b62b25de806bda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "e9a7a4cc47574386acfaef07ea88892c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "none",
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad2389d0a73c4b53aef201c9240bed5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "2881bd8e35f6442eaf3a4b8e601b69b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tiger-A/GoogleCollab/blob/main/%D0%92%D0%B5%D0%B1%D0%B8%D0%BD%D0%B0%D1%80_25_%D0%BC%D0%B0%D1%8F_%D0%9D%D0%B5%D1%80%D0%B9%D0%BE_%D0%BF%D1%80%D0%BE%D0%B4%D0%B0%D0%B6%D0%BD%D0%B8%D0%BA_2_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загружаем функции\n"
      ],
      "metadata": {
        "id": "CRMeSqo0Lpfw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MpHZVZxWgkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc7c3608-fdeb-4795-acae-a995c560f274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m873.5/873.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.6/922.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.30.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install --upgrade tiktoken\n",
        "!pip -q install langchain openai chromadb\n",
        "!pip -q install gspread oauth2client\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "import requests\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import re\n",
        "import logging\n",
        "logging.getLogger(\"langchain.text_splitter\").setLevel(logging.ERROR) # игнорирование предупреждений\n",
        "logging.getLogger(\"chromadb\").setLevel(logging.ERROR)\n",
        "import os\n",
        "import openai\n",
        "import tiktoken\n",
        "import re\n",
        "\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKCYAN = '\\033[96m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "\n",
        "def set_key():\n",
        "    password_input = widgets.Password(\n",
        "        description='Введите пароль:', \n",
        "        layout=widgets.Layout(width='500px'),\n",
        "        style={'description_width': 'initial', 'white-space': 'pre-wrap', 'overflow': 'auto'})\n",
        "    login_button = widgets.Button(description='Авторизация')\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def on_button_clicked(_):\n",
        "        with output:\n",
        "            openai.api_key = password_input.value\n",
        "            os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "            print(f'{bcolors.OKGREEN}{bcolors.BOLD}Ключ сохранен!{bcolors.ENDC}')\n",
        "            password_input.layout.display = 'none'\n",
        "            login_button.layout.display = 'none'\n",
        "\n",
        "    login_button.on_click(on_button_clicked)\n",
        "    display(widgets.VBox([password_input, login_button, output]))\n",
        "\n",
        "def load_document_text(url: str) -> str:\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the document as plain text\n",
        "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    response.raise_for_status()\n",
        "    text = response.text\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_search_index(text: str) -> Chroma:\n",
        "    return create_embedding(text)\n",
        "\n",
        "\n",
        "def load_prompt(url: str) -> str:\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the document as plain text\n",
        "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    response.raise_for_status()\n",
        "    text = response.text\n",
        "    return f'{text}'\n",
        "\n",
        "\n",
        "def create_embedding(data):\n",
        "    def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "        \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        num_tokens = len(encoding.encode(string))\n",
        "        return num_tokens\n",
        "\n",
        "    source_chunks = []\n",
        "    splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1024, chunk_overlap=0)\n",
        "\n",
        "    for chunk in splitter.split_text(data):\n",
        "        source_chunks.append(Document(page_content=chunk, metadata={}))\n",
        "\n",
        "    # Создание индексов документа\n",
        "    search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings(), )\n",
        "\n",
        "    count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), \"cl100k_base\")\n",
        "    #print('\\n ===========================================: ')\n",
        "    #print('Количество токенов в документе :', count_token)\n",
        "    #print('ЦЕНА запроса:', 0.0004*(count_token/1000), ' $')\n",
        "    return search_index\n",
        "\n",
        "def answer(system, topic, temp = 1):    \n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": system},\n",
        "      {\"role\": \"user\", \"content\": topic}\n",
        "      ]\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=messages,\n",
        "      temperature=temp\n",
        "      )\n",
        "\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "    \"\"\"\n",
        "    Возвращает количество токенов, используемых списком сообщений.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Пытаемся получить кодировку для выбранной модели\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        # Если кодировка для выбранной модели не найдена, используем кодировку \"cl100k_base\"\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    # Если выбранная модель это \"gpt-3.5-turbo-0301\"\n",
        "    if model == \"gpt-3.5-turbo-0301\": \n",
        "        # Инициализируем счетчик токенов\n",
        "        num_tokens = 0\n",
        "        # Проходимся по каждому сообщению в списке сообщений\n",
        "        for message in messages:\n",
        "            # Каждое сообщение обрамляется токенами <im_start> и <im_end>, а также символами новой строки, всего 4 токена\n",
        "            num_tokens += 4 \n",
        "            # Проходимся по каждому полю в сообщении (ключ и значение)\n",
        "            for key, value in message.items():\n",
        "                # Считаем количество токенов в значении и добавляем их в счетчик токенов\n",
        "                num_tokens += len(encoding.encode(value))\n",
        "                # Если ключ это \"name\", то это означает что роль (role) опущена\n",
        "                if key == \"name\":  \n",
        "                    # Роль всегда требуется и всегда занимает 1 токен, так что вычитаем 1 из счетчика\n",
        "                    num_tokens += -1  \n",
        "        # Каждый ответ начинается с токена <im_start>assistant, так что добавляем 2 в счетчик\n",
        "        num_tokens += 2  \n",
        "        # Возвращаем количество токенов\n",
        "        return num_tokens\n",
        "    else:\n",
        "        # Если выбранная модель не поддерживается, генерируем исключение\n",
        "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
        "See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
        "\n",
        "\n",
        "def insert_newlines(text: str, max_len: int = 170) -> str:\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    current_line = \"\"\n",
        "    for word in words:\n",
        "        if len(current_line + \" \" + word) > max_len:\n",
        "            lines.append(current_line)\n",
        "            current_line = \"\"\n",
        "        current_line += \" \" + word\n",
        "    lines.append(current_line)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def dialog():\n",
        "    user = ''\n",
        "    dialog = ''\n",
        "\n",
        "    print(f'{bcolors.OKBLUE}{bcolors.BOLD}С чем связан ваш интерес к искусственному интеллекту?{bcolors.ENDC}')\n",
        "\n",
        "    while user.lower() not in ['stop', 'exit', 'выход']:\n",
        "        user = input('Клиент: ')\n",
        "        if user == 'Stop': break\n",
        "\n",
        "        dialog += '\\n\\n' + 'Клиент: ' + user\n",
        "        add_dialog = answer(expert_prompt, user)\n",
        "        \n",
        "        dialog += '\\n\\n' + 'Менеджер: ' + add_dialog      \n",
        "        print(f'\\n{bcolors.OKBLUE}{bcolors.BOLD}Менеджер:{bcolors.ENDC} {insert_newlines(add_dialog)}')\n",
        "        report = answer(validation_prompt, dialog)\n",
        "        answer_text = answer(action_prompt, report)\n",
        "\n",
        "        print(f'\\n{bcolors.OKGREEN}{bcolors.BOLD}Отчёт системы:\\n {bcolors.ENDC}{report}')\n",
        "        print(f'\\n{bcolors.HEADER}{bcolors.BOLD}Менеджер: {bcolors.ENDC}{insert_newlines(answer_text)}\\n\\n')\n",
        "\n",
        "    return dialog\n",
        "\n",
        "def answer_index(system, topic, search_index, temp=1, verbose=0):\n",
        "    \n",
        "    # Selecting documents similar to the question \n",
        "    docs = search_index.similarity_search(topic, k=5)\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "    message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\\nОтрывок документа №{i+1}\\n=====================' + doc.page_content + '\\n' for i, doc in enumerate(docs)]))\n",
        "    if verbose: print('message_content :\\n ======================================== \\n', message_content)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system + f\"{message_content}\"},\n",
        "        {\"role\": \"user\", \"content\": topic}\n",
        "    ]\n",
        "\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "    if verbose: print(f\"{num_tokens_from_messages(messages, 'gpt-3.5-turbo-0301')} tokens used for the question\")\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=temp\n",
        "    )\n",
        "\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "    if verbose: print(f'{completion[\"usage\"][\"total_tokens\"]} total tokens used (question-answer).')\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "    answer = insert_newlines(completion.choices[0].message.content)\n",
        "    #print('ANSWER : \\n', answer)\n",
        "    return answer  # возвращает ответ \n",
        "\n",
        "def get_chatgpt_ansver3(system, topic, search_index, temp = 1):\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": topic}\n",
        "    ]\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=temp\n",
        "    )\n",
        "    print('Ответ менеджера : \\n', insert_newlines(completion.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_history(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
        "        num_tokens = 0\n",
        "        for message in messages:\n",
        "            question, answer = message  # распаковываем кортеж\n",
        "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "            num_tokens += len(encoding.encode(question))\n",
        "            num_tokens += len(encoding.encode(answer))\n",
        "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "        return num_tokens\n",
        "    else:\n",
        "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\"\"\")"
      ],
      "metadata": {
        "id": "PIxHCagPzGUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_questions(dialog):\n",
        "    # Применяем модель GPT-3 для суммаризации вопросов\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Ты - ассистент отдела продаж, основанный на AI. Ты умеешь профессионально суммаризировать присланные тебе диалоги менеджера и клиента. Твоя задача - суммаризировать диалог, который тебе пришел.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Суммаризируй следующий диалог менеджера по продажам и клиента: \" + \" \".join(dialog)}\n",
        "    ]\n",
        "    \n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0.3,  # Используем более низкую температуру для более определенной суммаризации\n",
        "        max_tokens=500  # Ограничиваем количество токенов для суммаризации\n",
        "    )\n",
        "    \n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "sZO6AOKj-ZoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_user_question(system_doc_url: str, knowledge_base_url: str, user_question: str) -> str:\n",
        "    global question_history\n",
        "    system_doc_text = load_document_text(system_doc_url)\n",
        "    knowledge_base_text = load_document_text(knowledge_base_url)\n",
        "\n",
        "    # Создаем индексы поиска\n",
        "    knowledge_base_index = create_search_index(knowledge_base_text)\n",
        "\n",
        "    # Если в истории более одного вопроса, применяем суммаризацию\n",
        "    summarized_history = \"\"\n",
        "    if len(question_history) > 0:\n",
        "        summarized_history = \"Вот краткий обзор предыдущего диалога: \" + summarize_questions([q + ' ' + (a if a is not None else '') for q, a in question_history])\n",
        "\n",
        "    # Добавляем явное разделение между историей диалога и текущим вопросом\n",
        "    input_text =summarized_history + \"\\n\\nТекущий вопрос: \" + user_question\n",
        "\n",
        "    # Извлечение наиболее похожих отрезков текста из базы знаний и получение ответа модели\n",
        "    answer_text = answer_index(system_doc_text, input_text, knowledge_base_index, temp=temperature, verbose=verbose)\n",
        "    \n",
        "    # Добавляем вопрос пользователя и ответ системы в историю\n",
        "    question_history.append((user_question, answer_text if answer_text is not None else ''))\n",
        "\n",
        "    # Выводим суммаризированный текст, который видит модель\n",
        "    if summarized_history != \"\":\n",
        "        print(\"Вот суммаризированный текст, который видит модель:\\n\", summarized_history)\n",
        "    \n",
        "    return answer_text\n"
      ],
      "metadata": {
        "id": "w2wqS94z376f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_user_question_dialog(system_doc_url: str, knowledge_base_url: str, user_question: str, question_history: list) -> str:\n",
        "    \n",
        "    system_doc_text = load_document_text(system_doc_url)\n",
        "    knowledge_base_text = load_document_text(knowledge_base_url)\n",
        "\n",
        "    # Создаем индексы поиска\n",
        "    knowledge_base_index = create_search_index(knowledge_base_text)\n",
        "\n",
        "    # Если в истории более одного вопроса, применяем суммаризацию\n",
        "    summarized_history = \"\"\n",
        "    if len(question_history) > 0:\n",
        "        summarized_history = \"Вот краткий обзор предыдущего диалога: \" + summarize_questions([q + ' ' + (a if a is not None else '') for q, a in question_history])\n",
        "\n",
        "    # Добавляем явное разделение между историей диалога и текущим вопросом\n",
        "    input_text =summarized_history + \"\\n\\nТекущий вопрос: \" + user_question\n",
        "\n",
        "    # Извлечение наиболее похожих отрезков текста из базы знаний и получение ответа модели\n",
        "    answer_text = answer_index(system_doc_text, input_text, knowledge_base_index, temp=temperature, verbose=verbose)\n",
        "    \n",
        "    # Добавляем вопрос пользователя и ответ системы в историю\n",
        "    question_history.append((user_question, answer_text if answer_text is not None else ''))\n",
        "\n",
        "    # Выводим суммаризированный текст, который видит модель\n",
        "    if summarized_history != \"\":\n",
        "        print(summarized_history)\n",
        "    \n",
        "    return answer_text\n"
      ],
      "metadata": {
        "id": "QnFzY5jdMcbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dialog(system_doc_url, knowledge_base_url):\n",
        "    question_history = []\n",
        "    dialog = \"\"\n",
        "    while True:\n",
        "        user_question = input('Клиент: ')\n",
        "        if user_question.lower() == 'stop':\n",
        "            break\n",
        "        answer = answer_user_question_dialog(system_doc_url, knowledge_base_url, user_question, question_history)\n",
        "        dialog += f'\\nКлиент: {user_question} \\n Менеджер: {answer}'\n",
        "        print('\\nМенеджер: ', answer)\n",
        "\n",
        "    return "
      ],
      "metadata": {
        "id": "C7pFIH_iMi3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вводим ключ и запускаем диалог"
      ],
      "metadata": {
        "id": "b3Ehj-xWL4FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature=1\n",
        "verbose=0\n",
        "question_history = []"
      ],
      "metadata": {
        "id": "xitFJ2x2H_H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base_url = 'https://docs.google.com/document/d/1P87cjAIJ3etAteDKjpyZycxXlFaC1EcgLE9UxhrIlMY' # база знаний"
      ],
      "metadata": {
        "id": "SvutKFmX374G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_doc_url = 'https://docs.google.com/document/d/1vT5CNdoryrcP6Keqf5y7jauSj0Ru797xvpjTu9pZ0wo' # инструкция"
      ],
      "metadata": {
        "id": "Okg-9FITHPGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_key()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35,
          "referenced_widgets": [
            "fca46282be1e40018388ea9ca2342add",
            "f846d2a7a9c5428b9f032d12acabe04e",
            "b33ce174730f48b7a532f88ddf122200",
            "6014189eb6a74ea1b00166746d60a216",
            "bfe8bf8a264c41c98bed89313e893c32",
            "ac313144d8fc4943a81cbb48f38a7224",
            "f564498ffeb944269b62b25de806bda7",
            "e9a7a4cc47574386acfaef07ea88892c",
            "ad2389d0a73c4b53aef201c9240bed5f",
            "2881bd8e35f6442eaf3a4b8e601b69b5"
          ]
        },
        "id": "7sNLAjrQZPeo",
        "outputId": "7d10620c-d996-4c27-b0cd-cb7595ae5d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Password(description='Введите пароль:', layout=Layout(width='500px'), style=DescriptionStyle(de…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fca46282be1e40018388ea9ca2342add"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_dialog(system_doc_url, knowledge_base_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv42o87_kzq3",
        "outputId": "cd1b1dec-f310-4834-d990-6dc559a948ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Клиент: Привет. Меня зовут Света. Смотрела ваши реалити. Не совсем понятно, смогу ли я\n",
            "\n",
            "Менеджер:   Здравствуйте, Света! Расскажите, пожалуйста, для чего вы хотели бы изучать AI? Это поможет мне подобрать наиболее подходящий вариант обучения для вас. Кроме того, наша\n",
            " компания предлагает обучение с нуля, поэтому даже если вы не имеете предыдущего опыта в этой области, мы сможем помочь вам стать AI-разработчиком. Если вы готовы начать\n",
            " обучение, предлагаю вам сейчас зарегистрироваться на нашем курсе. Наши опытные преподаватели помогут вам узнать и овладеть технологиями AI уже через несколько дней после\n",
            " начала обучения.\n",
            "Клиент: ой, нет, думаю, у меня не получится. Я никогда не программировала, и математику не помню совсем, я ее только в школе проходила 100 лет назад\n",
            "Вот краткий обзор предыдущего диалога: Клиент по имени Света интересуется изучением AI, но не уверена, сможет ли она это сделать. Менеджер предлагает ей обучение с нуля и помощь в становлении AI-разработчиком. Он также предлагает ей зарегистрироваться на курсе и обещает, что опытные преподаватели помогут ей овладеть технологиями AI уже через несколько дней после начала обучения.\n",
            "\n",
            "Менеджер:   Понимаю ваши опасения, но наш курс рассчитан именно на тех, кто хочет начать заниматься AI с недостаточным опытом и знаниями в этой области. Кроме того, математика - это\n",
            " не самое главное для начала изучения AI, мы поможем вам разобраться с теорией и навыками программирования. Я рекомендую вам зарегистрироваться на курсе и начать обучение\n",
            " уже сегодня. Предлагаю прямо сейчас оплатить и забронировать место в группе. Как смотрите на это?\n",
            "Клиент: Боюсь, я одна не справлюсь. Кто-то сможет со мной индивидуально позаниматься при необходимости?\n",
            "Вот краткий обзор предыдущего диалога: Клиент по имени Света интересуется возможностью изучения AI, но выражает опасения из-за отсутствия опыта в программировании и знаний в математике. Менеджер по продажам убеждает ее, что курс рассчитан на новичков и поможет ей разобраться с теорией и программированием. Он предлагает ей зарегистрироваться на курсе и начать обучение уже сегодня.\n",
            "\n",
            "Менеджер:   Да, конечно, у нас в курсе предусмотрены тим-лидеры, которые будут работать с вами индивидуально и отвечать на вопросы. Вы сможете получать помощь и поддержку на каждом\n",
            " этапе обучения. Кроме того, у нас есть специальные группы в социальных сетях, где вы сможете общаться со своими коллегами и делиться опытом. Предлагаю прямо сейчас\n",
            " зарегистрироваться на курс и начать учиться вместе с другими студентами. Как смотрите на это предложение?\n",
            "Клиент: думаю, я не смогу быстро проходить занятия. Мне понадобится больше времени, чем другим студентам. Что будет, если я не буду успевать?\n",
            "Вот краткий обзор предыдущего диалога: Клиент по имени Света интересуется возможностью изучения AI, но выражает опасения из-за отсутствия опыта в программировании и знаний в математике. Менеджер по продажам убеждает ее, что курс рассчитан на начинающих и поможет ей освоить необходимые навыки. Он предлагает зарегистрироваться на курсе и начать обучение уже сегодня, а также обещает индивидуальную поддержку и помощь от тим-лидеров и коллег.\n",
            "\n",
            "Менеджер:   Мы понимаем, что каждый студент имеет свои особенности и темп обучения. По этой причине наши курсы спроектированы таким образом, чтобы студенты имели возможность\n",
            " самостоятельно выбирать темп и расписание занятий. Кроме того, если у вас возникнут какие-то трудности или вопросы, вы всегда можете обратиться к нашим тим-лидерам за\n",
            " помощью. Мы готовы предоставить вам всю необходимую поддержку, чтобы вы смогли успешно завершить курс и получить нужные знания. Также вам доступна возможность продлить\n",
            " доступ к курсу, если понадобится больше времени для изучения материала. Не беспокойтесь, мы всегда готовы поддержать наших студентов. Предлагаю прямо сейчас\n",
            " зарегистрироваться на курс и начать изучение AI уже сегодня. Как вы смотрите на это?\n",
            "Клиент: Ну, допустим. А что, если я оплачу обучение, а потом пойму, что это не мое... как тогда быть? \n",
            "Вот краткий обзор предыдущего диалога: Клиент Света интересуется возможностью изучения AI, но беспокоится о своих знаниях в математике и программировании. Менеджер по продажам убеждает ее, что их курс рассчитан на тех, кто начинает изучать AI без опыта и знаний в этой области. Он также обещает индивидуальную поддержку и помощь на каждом этапе обучения, а также возможность выбирать темп и расписание занятий. Менеджер предлагает зарегистрироваться на курс и начать обучение уже сегодня.\n",
            "\n",
            "Менеджер:   Понимаю ваши опасения, но у нас есть возможность вернуть оплату в течение 14 дней с момента покупки, если вы решите, что обучение не соответствует вашим ожиданиям или не\n",
            " подходит вам. К тому же на первых этапах обучения мы предоставляем бесплатный вводный курс, чтобы вы могли оценить, подходит ли наше обучение вам. А как смотрите на то,\n",
            " чтобы уже сегодня зарегистрироваться на курс? Это позволит вам начать обучение и оценить, соответствует ли обучение вашим потребностям. Предлагаю прямо сейчас оплатить и\n",
            " забронировать место в группе.\n",
            "Клиент: хорошо, какие есть тарифы? можете ли предоставить рассрочку?\n",
            "Вот краткий обзор предыдущего диалога: Клиент Света интересуется обучением AI, но беспокоится, что не имеет достаточно знаний в этой области. Менеджер по продажам убеждает ее, что их курс рассчитан на тех, кто хочет начать заниматься AI с недостаточным опытом и знаниями. Кроме того, на курсе предусмотрены тим-лидеры, которые будут работать с ней индивидуально и отвечать на вопросы. Менеджер также уверяет, что курс спроектирован таким образом, чтобы студенты могли выбирать темп и расписание занятий, а также получать всю необходимую поддержку. Он предлагает зарегистрироваться на курсе и начать обучение уже сегодня. Кроме того, клиент может вернуть оплату в течение 14 дней с момента покупки, если обучение не соответствует ее ожиданиям или не подходит ей.\n",
            "\n",
            "Менеджер:   Здравствуйте Света! У нас есть несколько тарифов для обучения AI, в зависимости от ваших потребностей. Один из них - \"Продвинутый\" тариф, который включает в себя курсы\n",
            " по нейронным сетям, Python, продаже AI на заказ, AI директу, гарантию трудоустройства и 4 стажировки на выбор из разных проектов. Стоимость этого тарифа составляет 219\n",
            " 900 рублей. Кроме того, мы предлагаем рассрочку нашим клиентам. Например, Вы можете начать оплачивать обучение \"Продвинутый\" тариф в рассрочку на 3 года с первым\n",
            " платежом в размере 33 000 рублей. Могу ли я помочь вам определиться с тарифом и рассказать подробнее о рассрочке? И, кстати, я предлагаю зарегистрироваться на курсе\n",
            " прямо сейчас, чтобы забронировать свое место и начать обучение уже сегодня.\n",
            "Клиент: Какие есть еще тарифы, кроме продвинутого? сколько они стоят? там возможна рассрочка?\n",
            "Вот краткий обзор предыдущего диалога: Клиент Света интересуется обучением AI, но беспокоится о своих знаниях в математике и программировании. Менеджер уверяет ее, что курс рассчитан на тех, кто не имеет опыта в этой области, и предлагает зарегистрироваться на курсе уже сегодня. Он также говорит о том, что на курсе есть тим-лидеры, которые будут работать с ней индивидуально и отвечать на вопросы, а также возможность продлить доступ к курсу, если понадобится больше времени для изучения материала. Менеджер предлагает несколько тарифов для обучения AI и рассрочку на оплату. Он также уточняет, что есть возможность вернуть оплату в течение 14 дней с момента покупки, если обучение не соответствует ожиданиям.\n",
            "\n",
            "Менеджер:   Кроме тарифа \"Продвинутый\" мы также предлагаем тариф \"Базовый\", который стоит 99 900 рублей и включает в себя 32 базовых урока. На этот тариф также возможна рассрочка на\n",
            " 2-3 года по 2500 рублей в месяц. Также есть тариф \"AI под ключ\", который стоит 369 900 рублей и включает в себя все возможности обучения по AI, а также помощь в создании\n",
            " AI проекта. На этот тариф также возможна рассрочка на 3 года. Подробности о тарифах вы можете найти в документе с информацией о продуктах компании.\n",
            "Клиент: я  все же сомневаюсь, мне нужно посоветоваться со своими родственниками.\n",
            "Вот краткий обзор предыдущего диалога: Клиент Света интересуется обучением AI, но беспокоится о своих знаниях в программировании и математике. Менеджер по продажам убеждает ее, что курс рассчитан на начинающих и предлагает зарегистрироваться на курсе уже сегодня. Клиент также интересуется о возможности индивидуальной помощи и поддержки, на что менеджер отвечает, что у них есть тим-лидеры и группы в социальных сетях для общения со студентами. Клиент также задает вопросы о тарифах и рассрочке, на что менеджер предлагает несколько тарифов и возможность рассрочки на 2-3 года.\n",
            "\n",
            "Менеджер:   Понимаю ваше желание получить мнение близких людей. Могу ли я помочь вам чем-то еще? Может быть, у вас есть ещё какие-то вопросы или сомнения, которые мешают вам принять\n",
            " решение? Я готов ответить на любые вопросы и обсудить все возможные варианты оплаты. А если вы готовы купить курс прямо сейчас, я могу помочь вам зарегистрироваться на\n",
            " сайте и выбрать подходящий для вас тарифный план.\n",
            "Клиент: stop\n"
          ]
        }
      ]
    }
  ]
}